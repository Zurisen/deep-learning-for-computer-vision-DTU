{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DRIhx7PugJy3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import PIL.Image as Image\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35PhqXpWUZ7I"
   },
   "source": [
    "We always check that we are running on a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ic_gOv_pUZeB"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"The code will run on GPU.\")\n",
    "else:\n",
    "    print(\"The code will run on CPU. Go to Edit->Notebook Settings and choose GPU as the hardware accelerator\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sAj64PJYgJzC"
   },
   "source": [
    "We provide you with a class that can load the *hotdog/not hotdog* dataset you should use from /dtu/datasets1/02514/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4mUlnOuzgJzF"
   },
   "outputs": [],
   "source": [
    "class Hotdog_NotHotdog(torch.utils.data.Dataset):\n",
    "    def __init__(self, train, transform, data_path='/dtu/datasets1/02514/hotdog_nothotdog'):\n",
    "        'Initialization'\n",
    "        self.transform = transform\n",
    "        data_path = os.path.join(data_path, 'train' if train else 'test')\n",
    "        image_classes = [os.path.split(d)[1] for d in glob.glob(data_path +'/*') if os.path.isdir(d)]\n",
    "        image_classes.sort()\n",
    "        self.name_to_label = {c: id for id, c in enumerate(image_classes)}\n",
    "        self.image_paths = glob.glob(data_path + '/*/*.jpg')\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Returns the total number of samples'\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        'Generates one sample of data'\n",
    "        image_path = self.image_paths[idx]\n",
    "        \n",
    "        image = Image.open(image_path)\n",
    "        c = os.path.split(os.path.split(image_path)[0])[1]\n",
    "        y = self.name_to_label[c]\n",
    "        X = self.transform(image)\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JewkmhKlgJzN"
   },
   "source": [
    "Below is the simple way of converting the images to something that can be fed through a network.\n",
    "Feel free to use something other than $128\\times128$ images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZcilkL3dgJzP"
   },
   "outputs": [],
   "source": [
    "size = 128\n",
    "jitter=0.5\n",
    "train_transform1 = transforms.Compose([transforms.Resize((size, size)), \n",
    "                                      transforms.RandomRotation(degrees=(90)),\n",
    "                                      transforms.ColorJitter(jitter),\n",
    "                                    transforms.ToTensor()])\n",
    "train_transform2 = transforms.Compose([transforms.Resize((size, size)), \n",
    "                                      transforms.RandomRotation(degrees=(90)),\n",
    "                                      transforms.GaussianBlur(3),\n",
    "                                    transforms.ToTensor()])\n",
    "train_transform_orig = transforms.Compose([transforms.Resize((size, size)), \n",
    "                                    transforms.ToTensor()])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.Resize((size, size)), \n",
    "                                    transforms.RandomRotation(degrees=(90)),\n",
    "                                      transforms.ColorJitter(jitter),\n",
    "                                      transforms.GaussianBlur(3),\n",
    "                                    transforms.ToTensor()])\n",
    "batch_size = 64\n",
    "trainset_trans1 = Hotdog_NotHotdog(train=True, transform=train_transform1)\n",
    "trainset_trans2 = Hotdog_NotHotdog(train=True, transform=train_transform2)\n",
    "trainset_orig = Hotdog_NotHotdog(train=True, transform=train_transform_orig)\n",
    "all_train=trainset_trans1+trainset_trans2+trainset_orig\n",
    "\n",
    "train_loader = DataLoader(all_train, batch_size=batch_size, shuffle=True, num_workers=3)\n",
    "testset = Hotdog_NotHotdog(train=False, transform=test_transform)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ho-YRb6HgJzZ"
   },
   "source": [
    "Let's look at some images from our data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sm4Ara7dgJza"
   },
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "for i in range(21):\n",
    "    plt.subplot(5,7,i+1)\n",
    "    plt.imshow(np.swapaxes(np.swapaxes(images[i].numpy(), 0, 2), 0, 1))\n",
    "    plt.title(['hotdog', 'not hotdog'][labels[i].item()])\n",
    "    plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JgcJoEkzQ5da"
   },
   "source": [
    "Remember to save the state of your model AND optimizer regularly in case the Colab runtime times out.\n",
    "You can save your model to your google drive, so you can get it from there in a new colab session. \n",
    "\n",
    "If you only save it in the colab notebook, there's no way to get it into a new session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12N0EYYsQPhJ"
   },
   "source": [
    "Now create a model and train it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ResNet import ResNet152\n",
    "#resnet=ResNet152(2).to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we are going to use a very basic neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicNetwork(nn.Module):\n",
    "    def __init__(self, n_filters, out_features=2):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "                        nn.Conv2d(3, n_filters, kernel_size=4, stride=2, padding=1), \n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv2d(n_filters, n_filters*2, kernel_size=4, stride=2, padding=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                        nn.ReLU()\n",
    "        )\n",
    "        self.linear = nn.Linear(n_filters*2*8*8, 2)\n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        out = self.linear(out.view(x.size(0), -1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicNetwork_2(nn.Module):\n",
    "    \"\"\" BasicNetwork + Batchnorm \"\"\"\n",
    "    def __init__(self, n_filters, out_features=2):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "                        nn.Conv2d(3, n_filters, kernel_size=4, stride=2, padding=1), \n",
    "                        nn.BatchNorm2d(n_filters),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                        nn.BatchNorm2d(n_filters),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv2d(n_filters, n_filters*2, kernel_size=4, stride=2, padding=1),\n",
    "                        nn.BatchNorm2d(n_filters*2),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                        nn.BatchNorm2d(n_filters*2),\n",
    "                        nn.ReLU()\n",
    "        )\n",
    "        self.linear = nn.Linear(n_filters*2*8*8, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        out = self.linear(out.view(x.size(0), -1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicNetwork_3(nn.Module):\n",
    "    \"\"\" BasicNetwork + Batchnorm + Dropout \"\"\"\n",
    "    def __init__(self, n_filters, out_features=2):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "                        nn.Conv2d(3, n_filters, kernel_size=4, stride=2, padding=1), \n",
    "                        nn.BatchNorm2d(n_filters),\n",
    "                        nn.Dropout(0.3),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                        nn.BatchNorm2d(n_filters),\n",
    "                        nn.Dropout(0.3),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv2d(n_filters, n_filters*2, kernel_size=4, stride=2, padding=1),\n",
    "                        nn.BatchNorm2d(n_filters*2),\n",
    "                        nn.Dropout(0.3),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                        nn.BatchNorm2d(n_filters*2),\n",
    "                        nn.Dropout(0.3),\n",
    "                        nn.ReLU()\n",
    "        )\n",
    "        self.linear = nn.Linear(n_filters*2*8*8, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        out = self.linear(out.view(x.size(0), -1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We define the training as a function so we can easily re-use it.\n",
    "def train(model, optimizer, loss_fun, name, num_epochs=30, from_checkpoint=False):\n",
    "    \n",
    "    if from_checkpoint:\n",
    "        model.load_state_dict(torch.load(os.path.join(\"checkpoints\",name)))\n",
    "    \n",
    "    out_dict = {'train_acc': [],\n",
    "              'test_acc': [],\n",
    "              'train_loss': [],\n",
    "              'test_loss': [],\n",
    "              'predictions': [],\n",
    "              'target': []}\n",
    "  \n",
    "    for epoch in tqdm(range(num_epochs), unit='epoch'):\n",
    "        model.train()\n",
    "        #For each epoch\n",
    "        train_correct = 0\n",
    "        train_loss = []\n",
    "        model.train()\n",
    "        for minibatch_no, (data, target) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            #Zero the gradients computed for each weight\n",
    "            optimizer.zero_grad()\n",
    "            #Forward pass your image through the network\n",
    "            output = model(data)\n",
    "            #Compute the loss\n",
    "            loss = loss_fun(output, target)\n",
    "            \n",
    "            #Backward pass through the network\n",
    "            loss.backward()\n",
    "            #Update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "            #Compute how many were correctly classified\n",
    "            predicted = output.argmax(1)\n",
    "            train_correct += (target==predicted).sum().cpu().item()\n",
    "        #Comput the test accuracy\n",
    "        test_loss = []\n",
    "        test_correct = 0\n",
    "        model.eval()\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            with torch.no_grad():\n",
    "                output = model(data)\n",
    "            test_loss.append(loss_fun(output, target).cpu().item())\n",
    "            predicted = output.argmax(1)\n",
    "            test_correct += (target==predicted).sum().cpu().item()\n",
    "            out_dict[\"predictions\"].append(predicted)\n",
    "            out_dict[\"target\"].append(target)\n",
    "        out_dict['train_acc'].append(train_correct/len(all_train))\n",
    "        out_dict['test_acc'].append(test_correct/len(testset))\n",
    "        out_dict['train_loss'].append(np.mean(train_loss))\n",
    "        out_dict['test_loss'].append(np.mean(test_loss))\n",
    "        print(f\"Loss train: {np.mean(train_loss):.3f}\\t test: {np.mean(test_loss):.3f}\\t\",\n",
    "              f\"Accuracy train: {out_dict['train_acc'][-1]*100:.1f}%\\t test: {out_dict['test_acc'][-1]*100:.1f}%\")\n",
    "        \n",
    "        # Saving weights\n",
    "        torch.save(model.state_dict(), os.path.join(\"checkpoints\",name))\n",
    "        \n",
    "        # Saving training/testing info\n",
    "        info_df = pd.DataFrame()\n",
    "        info_df['train_acc'] = np.array(out_dict['train_acc']).reshape(-1)\n",
    "        info_df['test_acc'] = np.array(out_dict['test_acc']).reshape(-1)\n",
    "        info_df['train_loss'] = np.array(out_dict['train_loss']).reshape(-1)\n",
    "        info_df['test_loss'] = np.array(out_dict['test_loss']).reshape(-1)\n",
    "        info_df.to_csv(name+\"_info.csv\", index=False)\n",
    "        \n",
    "        \n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our networks\n",
    "basicnetwork = BasicNetwork(4).to(device)\n",
    "basicnetwork2 = BasicNetwork_2(4).to(device)\n",
    "basicnetwork3 = BasicNetwork_3(4).to(device)\n",
    "\n",
    "# Some state of the art networks\n",
    "resnet152 = models.resnet152(num_classes=2, pretrained=False).to(device)\n",
    "alexnet = models.alexnet(num_classes=2, pretrained=False).to(device)\n",
    "vgg16 = models.vgg16(num_classes=2, pretrained=False).to(device)\n",
    "\n",
    "# CrossE ntropy Loss function\n",
    "loss_fun = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SGD and adam optimizers\n",
    "optimizer_SGD = optim.SGD(basicnetwork.parameters(), lr=0.01)\n",
    "optimizer_Adam = optim.Adam(basicnetwork.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict = train(resnet152, optimizer_SGD, loss_fun, \"ResNet152_SGD\", num_epochs=50)\n",
    "out_dict = train(alexnet, optimizer_SGD, loss_fun, \"AlexNet_SGD\", num_epochs=50)\n",
    "out_dict = train(vgg16, optimizer_SGD, loss_fun, \"VGG16_SGD\", num_epochs=50)\n",
    "\n",
    "out_dict = train(resnet152, optimizer_Adam, loss_fun, \"ResNet152_Adam\", num_epochs=50)\n",
    "out_dict = train(alexnet, optimizer_Adam, loss_fun, \"AlexNet_Adam\", num_epochs=50)\n",
    "out_dict = train(vgg16, optimizer_Adam, loss_fun, \"VGG16_Adam\", num_epochs=50)\n",
    "\n",
    "out_dict = train(basicnetwork, optimizer_SGD, loss_fun, \"BasicNetwork_SGD\", num_epochs=50)\n",
    "out_dict = train(basicnetwork2, optimizer_SGD, loss_fun, \"BasicNetwork_2_SGD\", num_epochs=50)\n",
    "out_dict = train(basicnetwork3, optimizer_SGD, loss_fun, \"BasicNetwork_3_SGD\", num_epochs=50)\n",
    "\n",
    "out_dict = train(basicnetwork, optimizer_Adam, loss_fun, \"BasicNetwork_Adam\", num_epochs=50)\n",
    "out_dict = train(basicnetwork2, optimizer_Adam, loss_fun, \"BasicNetwork_2_Adam\", num_epochs=50)\n",
    "out_dict = train(basicnetwork3, optimizer_Adam, loss_fun, \"BasicNetwork_3_Adam\", num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Project 1.1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
